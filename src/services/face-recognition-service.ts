// src/services/face-recognition.ts
import { api } from "@/lib/axios";
import * as faceapi from "face-api.js";

interface ImageData {
  url: string;
  path: string;
}

export interface VerificationResult {
  isMatch: boolean;
  similarity: number;
  confidence: number;
  label: string;
  processingTime: number;
}

class FaceRecognitionService {
  private isInitialized = false;
  private readonly SIMILARITY_THRESHOLD = 0.6; // 60% de similaridade m√≠nima
  private readonly CONFIDENCE_THRESHOLD = 0.7; // 70% de confian√ßa m√≠nima

  async initialize() {
    if (this.isInitialized) return;

    try {
      console.log("üöÄ Iniciando carregamento dos modelos face-api.js...");
      const MODEL_URL = "/models";
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
      ]);

      console.log("‚úÖ Modelos carregados com sucesso!");
      this.isInitialized = true;
    } catch (error) {
      console.error("‚ùå Erro ao carregar modelos:", error);
      throw error;
    }
  }

  async loadReferenceImagesById(
    id: string,
  ): Promise<faceapi.LabeledFaceDescriptors[]> {
    try {
      console.log(`üì° Buscando imagens de refer√™ncia para ID ${id}...`);

      const { data } = await api.get<{ images: ImageData[] }>(
        `/face/images/${id}`,
      );
      console.log("üì∏ Imagens encontradas:", data.images?.length || 0);

      const labeledDescriptors = await Promise.all(
        data.images.map(async (image) => {
          try {
            const imageResponse = await api.get(image.url, {
              responseType: "blob",
            });
            const imgBlob = imageResponse.data;
            const img = await faceapi.bufferToImage(imgBlob);

            const detection = await faceapi
              .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
              .withFaceLandmarks()
              .withFaceDescriptor();

            if (!detection) {
              console.warn(`‚ö†Ô∏è Nenhum rosto detectado em: ${image.path}`);
              return null;
            }

            return new faceapi.LabeledFaceDescriptors(id, [
              detection.descriptor,
            ]);
          } catch (error) {
            console.error(`‚ùå Erro ao processar imagem ${image.path}:`, error);
            return null;
          }
        }),
      );

      const validDescriptors = labeledDescriptors.filter(
        (desc): desc is faceapi.LabeledFaceDescriptors => desc !== null,
      );
      console.log(
        "‚úÖ Descritores v√°lidos carregados:",
        validDescriptors.length,
      );

      return validDescriptors;
    } catch (error) {
      console.error("‚ùå Erro ao carregar imagens de refer√™ncia:", error);
      throw error;
    }
  }

  async verifyFace(
    capturedImage: string,
    funcionarioId: string,
  ): Promise<VerificationResult> {
    const startTime = performance.now();

    try {
      await this.initialize();

      // Carrega as imagens de refer√™ncia do funcion√°rio
      const referenceDescriptors =
        await this.loadReferenceImagesById(funcionarioId);

      if (referenceDescriptors.length === 0) {
        throw new Error(
          "Nenhuma imagem de refer√™ncia encontrada para o funcion√°rio",
        );
      }

      // Converte a imagem capturada para o formato aceito pelo face-api.js
      const img = await faceapi.bufferToImage(
        await (await fetch(capturedImage)).blob(),
      );

      // Detecta o rosto na imagem capturada
      const detection = await faceapi
        .detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptor();

      if (!detection) {
        throw new Error("Nenhum rosto detectado na imagem capturada");
      }

      // Cria o matcher com as imagens de refer√™ncia
      const faceMatcher = new faceapi.FaceMatcher(
        referenceDescriptors,
        this.SIMILARITY_THRESHOLD,
      );

      // Faz o match do rosto capturado com as refer√™ncias
      const match = faceMatcher.findBestMatch(detection.descriptor);

      const processingTime = performance.now() - startTime;

      return {
        isMatch: match.label !== "unknown",
        similarity: (1 - match.distance) * 100, // Converte para porcentagem
        confidence: this.CONFIDENCE_THRESHOLD * 100,
        label: match.label,
        processingTime,
      };
    } catch (error) {
      console.error("‚ùå Erro na verifica√ß√£o facial:", error);
      throw error;
    }
  }

  async verifyMultipleFaces(
    capturedImages: string[],
    funcionarioId: string,
  ): Promise<VerificationResult> {
    try {
      console.log(
        `üîç Iniciando verifica√ß√£o de ${capturedImages.length} imagens...`,
      );

      const results = await Promise.all(
        capturedImages.map((img) => this.verifyFace(img, funcionarioId)),
      );

      // Calcula a m√©dia dos resultados
      const averageResult = results.reduce(
        (acc, curr) => {
          return {
            isMatch: acc.isMatch && curr.isMatch,
            similarity: acc.similarity + curr.similarity / results.length,
            confidence: this.CONFIDENCE_THRESHOLD * 100,
            label: curr.label,
            processingTime:
              acc.processingTime + curr.processingTime / results.length,
          };
        },
        {
          isMatch: true,
          similarity: 0,
          confidence: this.CONFIDENCE_THRESHOLD * 100,
          label: funcionarioId,
          processingTime: 0,
        },
      );

      console.log("‚úÖ Verifica√ß√£o m√∫ltipla conclu√≠da:", averageResult);
      return averageResult;
    } catch (error) {
      console.error("‚ùå Erro na verifica√ß√£o m√∫ltipla:", error);
      throw error;
    }
  }
}

export const faceRecognitionService = new FaceRecognitionService();
